{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives-for-this-lecture\" data-toc-modified-id=\"Objectives-for-this-lecture-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives for this lecture</a></span></li><li><span><a href=\"#Introducing-Tensorflow-and-Keras\" data-toc-modified-id=\"Introducing-Tensorflow-and-Keras-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introducing Tensorflow and Keras</a></span></li><li><span><a href=\"#VGG-16\" data-toc-modified-id=\"VGG-16-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>VGG-16</a></span></li><li><span><a href=\"#Loading-VGG-16\" data-toc-modified-id=\"Loading-VGG-16-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Loading VGG-16</a></span><ul class=\"toc-item\"><li><span><a href=\"#Make-a-prediction-using-an-image\" data-toc-modified-id=\"Make-a-prediction-using-an-image-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Make a prediction using an image</a></span></li></ul></li><li><span><a href=\"#Transfer-Learning\" data-toc-modified-id=\"Transfer-Learning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Transfer Learning</a></span></li><li><span><a href=\"#Example-of-Transfer-Learning\" data-toc-modified-id=\"Example-of-Transfer-Learning-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Example of Transfer Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-load-the-pre-trained-model\" data-toc-modified-id=\"Step-1:-load-the-pre-trained-model-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Step 1: load the pre-trained model</a></span></li><li><span><a href=\"#Step-2:-download-the-new-training-images\" data-toc-modified-id=\"Step-2:-download-the-new-training-images-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Step 2: download the new training images</a></span></li><li><span><a href=\"#Step-3:-download-the-new-labels\" data-toc-modified-id=\"Step-3:-download-the-new-labels-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Step 3: download the new labels</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives for this lecture\n",
    "\n",
    "After today, you will know how to:\n",
    "* Load a pre-trained CNN image classifier\n",
    "* Classify your own image using a pre-trained CNN\n",
    "* Download remote sensing data from Google Earth Engine\n",
    "* Build a transfer learning model to make predictions from satellite images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Tensorflow and Keras\n",
    "\n",
    "**Tensorflow** is Google's machine learning library. Tensorflow is very powerful, and it is easy to get overwhelmed if you aren't familiar with it. \n",
    "\n",
    "**Keras** is a library that serves as a \"wrapper\" for Tensorflow. A wrapper is a set of high-level functions that make it easy to accomplish basic tasks without writing a lot of code. Keras is written for both Python and R.\n",
    "\n",
    "![](notebook_images/keras_tensorflow.jpeg)\n",
    "\n",
    "For this exercise, we will use Keras. \n",
    "\n",
    "# VGG-16\n",
    "\n",
    "VGG-16 is the name of a convolutional neural network (CNN) that was developed by the Oxford Visual Geometry Group (VGG). The model has 16 layers, hence the name VGG-16. \n",
    "\n",
    "Consider the model below, which is taken from the slides in class. The model below has 4 layers (convolution, pooling, convolution, pooling). VGG-16 is a similar concept, except VGG-16 has 16 layers instead of 4.\n",
    "\n",
    "![](notebook_images/simple_model.png)\n",
    "\n",
    "VGG-16 became famous when it won the ImageNet Large Scale Visual Recognition Challenge in 2014. The challenge is to see who can build the best image classifier for a very large set (1000 classes) of images. The model above classifies images into 10 classes (you can see the 10 output classes as red dots on the right side). Imagine having 1000 classes - that is what the VGG-16 was trained to do. Let's take a look at some example images from the ImageNet dataset. \n",
    "\n",
    "![Samples from ImageNet](notebook_images/ImageNet.png)\n",
    "\n",
    "Each image has a label. For example, the image of pizza slices is labeled \"pizza\". VGG-16 was trained to classify each image into 1 of the 1000 classes with only 7% classification error. Not bad! (Actually, 7% error refers to the *top 5* error rate, which means that the correct label was in the top 5 model predictions). There are better models available now, but the prediction accuracy isn't much higher than that of the VGG-16. \n",
    "\n",
    "The VGG group from Oxford has made the pre-trained model freely available for anyone to download and use. This means that we can load the model ourselves and theoretically classify ImageNet images with 93% accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading VGG-16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must have tensorflow and keras installed in order to execute the following code. [This link](https://www.tensorflow.org/install/) shows you how to install tensorflow using pip. You can also install it from Anaconda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the model. This may take a minute because the model weights file is pretty large. \n",
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG16(weights=\"imagenet\")\n",
    "\n",
    "# Look at the model architecture. There should be 16 layers.\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not have to use the pre-trained ImageNet weights. We could also just load the VGG-16 architecture and then train it ourselves. However, training a model from scratch requires a lot of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction using an image\n",
    "\n",
    "Let's load a new image and see how the model classifies it. Here is an image of Professor Carlson. How do you think the model will classify this image? \n",
    "\n",
    "![](notebook_images/carlson.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to pre-process the image so that it is in the format that VGG-16 expects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# load an image from file\n",
    "image = load_img('notebook_images/carlson.jpg', target_size=(224, 224))\n",
    "\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "\n",
    "# reshape data for the model (add a third dimension)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "# the training images were pre-processed a bit. This function applies the same pre-processing to the test image\n",
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, make a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suit (76.90%)\n",
      "Windsor_tie (16.07%)\n",
      "bow_tie (1.67%)\n",
      "oboe (0.98%)\n",
      "groom (0.71%)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "# predict the probability across all output classes\n",
    "prediction = model.predict(image)\n",
    "\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(prediction)\n",
    "\n",
    "# retrieve the top 5 most likely results, e.g. top 5 highest probabilities\n",
    "label = label[0][0:5]\n",
    "\n",
    "# print the classifications for the top 5 predictions\n",
    "print('%s (%.2f%%)' % (label[0][1], label[0][2]*100))\n",
    "print('%s (%.2f%%)' % (label[1][1], label[1][2]*100))\n",
    "print('%s (%.2f%%)' % (label[2][1], label[2][2]*100))\n",
    "print('%s (%.2f%%)' % (label[3][1], label[3][2]*100))\n",
    "print('%s (%.2f%%)' % (label[4][1], label[4][2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is 76.9% certain that the image corresponds to \"suit\". There probably is not an ImageNet class called \"data science professor\". \n",
    "\n",
    "We now have successfully utilized a pre-trained CNN for the purpose that it was orginally designed for (i.e., classifying \"typical\" images).\n",
    "\n",
    "What if we wanted to use the pre-trained model for a task other than the one it was designed for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfer learning is process of taking useful knowledge from one problem and transfering it to another problem. Transfer learning is common practice for developing CNNs because rarely does one have enough data to train the network from scratch. If we try to train a network with too few observations, then the model will overfit and will not generalize to the population. \n",
    "\n",
    "Instead, we can use the weights from a pre-trained model (like VGG-16 with ImageNet weights) and then re-train some (but not all) of the layers to suit our needs. \n",
    "\n",
    "For example, suppose we wanted to predict air quality from satellite images of cities. Satellite images of cities look different than the ImageNet images (pizza, cats, etc.), but not *that* different. After all, they're still photographs, right? The low-level features that correspond to basic elements of images (lines, edges, etc.) are still relevent for the new task. So, what we could do is keep *most* of the ImageNet weights and then train *some* upper-level layers to specialize the model to the new task. \n",
    "\n",
    "Let's see how it looks if we load the pre-trained VGG-16 but allow the top 3 layers to be specialized to the new task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 9,439,232\n",
      "Non-trainable params: 5,275,456\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(weights = 'imagenet', include_top = True)\n",
    "\n",
    "# Freeze the layers which you don't want to train. Here I am freezing the first 13 layers.\n",
    "for layer in model.layers[:13]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that now we have non-trainable parameters as well as trainable parameters. We are *transferring* the first 13 layers from the ImageNet model. The last 3 layers we can train to suit our needs. \n",
    "\n",
    "How many layers of a pre-trained model should we freeze for transfer learning? It depends on the task. \n",
    "\n",
    "If the new datast is **small**, then we should probably just train the last layer or two. Training the model with few observations will lead to overfitting. \n",
    "\n",
    "If the new dataset is **large**, then we can train as many layers as we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Transfer Learning\n",
    "\n",
    "Let's do a concrete example of a transfer learning for environmental engineering. Suppose we wanted to train a CNN to estimate the population density of a given neighborhood (like Duke campus) from satellite images. Population density can be useful for many environmental applications, such as catastrophe risk analysis or hedonic pricing analysis. \n",
    "\n",
    "**Note**: Unfortunately, we cannot actually train a new model during this demonstration because it would take too long! However, we will go over all of the steps so that you can use transfer learning for your own purposes.\n",
    "\n",
    "We need 3 things in order to train the model. \n",
    "* pre-trained CNN (with *some* layers trainable)\n",
    "* new images (satellite images)\n",
    "* new labels (population density)\n",
    "\n",
    "## Step 1: load the pre-trained model\n",
    "\n",
    "We already did this!\n",
    "\n",
    "## Step 2: download the new training images\n",
    "\n",
    "In this case, we want satellite images of neighborhoods. To do this, I first obtained neighborhood shapefiles from [Zillow](https://www.zillow.com/research/data/), then downloaded the corresponding NAIP (National Agriculture Imagery Program) images using [Google Earth Engine](https://earthengine.google.com/). \n",
    "\n",
    "(quick [tour](https://code.earthengine.google.com/2afafdd6e7f906f0d2d40f9ec0416bbd) of Google Earth Engine! Note that you will need a Google Earth Engine account to use this service. It's free.)\n",
    "\n",
    "## Step 3: download the new labels\n",
    "\n",
    "I downloaded population density data from the [CoolClimate network](https://coolclimate.org/), and then joined the data to the neighborhood shapefiles. \n",
    "\n",
    "Once you've downloaded your data (new images and new labels), the best thing to do is make a pandas dataframe in which each row corresponds to an observation. In our case, we would have one column called \"image\" and one column called \"popden\". The \"image\" column would contain strings that correspond to the filepath of the image. The \"popden\" column would be numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>RegionID</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>Population</th>\n",
       "      <th>PersonsPerHousehold</th>\n",
       "      <th>AverageHouseValue</th>\n",
       "      <th>IncomePerHousehold</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>...</th>\n",
       "      <th>State.y</th>\n",
       "      <th>County</th>\n",
       "      <th>City.y</th>\n",
       "      <th>Name</th>\n",
       "      <th>AFFGEOID10</th>\n",
       "      <th>GEOID10</th>\n",
       "      <th>ALAND10</th>\n",
       "      <th>AWATER10</th>\n",
       "      <th>image</th>\n",
       "      <th>image_remote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7787</td>\n",
       "      <td>27107</td>\n",
       "      <td>38788</td>\n",
       "      <td>2.61</td>\n",
       "      <td>93400</td>\n",
       "      <td>36279</td>\n",
       "      <td>36.060969</td>\n",
       "      <td>-80.184068</td>\n",
       "      <td>912</td>\n",
       "      <td>...</td>\n",
       "      <td>NC</td>\n",
       "      <td>Forsyth</td>\n",
       "      <td>Winston-Salem</td>\n",
       "      <td>Waughtown</td>\n",
       "      <td>8600000US27107</td>\n",
       "      <td>27107</td>\n",
       "      <td>168507642</td>\n",
       "      <td>1428803</td>\n",
       "      <td>/Users/Jon/Documents/NEWOS_DL/images/Zillow_NC...</td>\n",
       "      <td>images/Zillow_NC/Zillow7787.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8382</td>\n",
       "      <td>27006</td>\n",
       "      <td>10762</td>\n",
       "      <td>2.48</td>\n",
       "      <td>166200</td>\n",
       "      <td>51467</td>\n",
       "      <td>36.010380</td>\n",
       "      <td>-80.447340</td>\n",
       "      <td>782</td>\n",
       "      <td>...</td>\n",
       "      <td>NC</td>\n",
       "      <td>Davie</td>\n",
       "      <td>Advance</td>\n",
       "      <td>Bermuda Run</td>\n",
       "      <td>8600000US27006</td>\n",
       "      <td>27006</td>\n",
       "      <td>165428089</td>\n",
       "      <td>2643968</td>\n",
       "      <td>/Users/Jon/Documents/NEWOS_DL/images/Zillow_NC...</td>\n",
       "      <td>images/Zillow_NC/Zillow8382.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12870</td>\n",
       "      <td>28792</td>\n",
       "      <td>27062</td>\n",
       "      <td>2.38</td>\n",
       "      <td>111300</td>\n",
       "      <td>32820</td>\n",
       "      <td>35.337939</td>\n",
       "      <td>-82.450039</td>\n",
       "      <td>2146</td>\n",
       "      <td>...</td>\n",
       "      <td>NC</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Hendersonville</td>\n",
       "      <td>Mountain Home</td>\n",
       "      <td>8600000US28792</td>\n",
       "      <td>28792</td>\n",
       "      <td>260576005</td>\n",
       "      <td>716642</td>\n",
       "      <td>/Users/Jon/Documents/NEWOS_DL/images/Zillow_NC...</td>\n",
       "      <td>images/Zillow_NC/Zillow12870.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>19609</td>\n",
       "      <td>28213</td>\n",
       "      <td>25882</td>\n",
       "      <td>2.71</td>\n",
       "      <td>110600</td>\n",
       "      <td>41340</td>\n",
       "      <td>35.291316</td>\n",
       "      <td>-80.780822</td>\n",
       "      <td>721</td>\n",
       "      <td>...</td>\n",
       "      <td>NC</td>\n",
       "      <td>Mecklenburg</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Newell</td>\n",
       "      <td>8600000US28213</td>\n",
       "      <td>28213</td>\n",
       "      <td>35846902</td>\n",
       "      <td>231444</td>\n",
       "      <td>/Users/Jon/Documents/NEWOS_DL/images/Zillow_NC...</td>\n",
       "      <td>images/Zillow_NC/Zillow19609.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>24954</td>\n",
       "      <td>28601</td>\n",
       "      <td>46176</td>\n",
       "      <td>2.41</td>\n",
       "      <td>122200</td>\n",
       "      <td>42046</td>\n",
       "      <td>35.753580</td>\n",
       "      <td>-81.324900</td>\n",
       "      <td>969</td>\n",
       "      <td>...</td>\n",
       "      <td>NC</td>\n",
       "      <td>Catawba</td>\n",
       "      <td>Hickory</td>\n",
       "      <td>Green Park</td>\n",
       "      <td>8600000US28601</td>\n",
       "      <td>28601</td>\n",
       "      <td>119209116</td>\n",
       "      <td>8448396</td>\n",
       "      <td>/Users/Jon/Documents/NEWOS_DL/images/Zillow_NC...</td>\n",
       "      <td>images/Zillow_NC/Zillow24954.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  RegionID  ZipCode  Population  PersonsPerHousehold  \\\n",
       "0           1      7787    27107       38788                 2.61   \n",
       "1           2      8382    27006       10762                 2.48   \n",
       "2           3     12870    28792       27062                 2.38   \n",
       "3           4     19609    28213       25882                 2.71   \n",
       "4           5     24954    28601       46176                 2.41   \n",
       "\n",
       "   AverageHouseValue  IncomePerHousehold   Latitude  Longitude  Elevation  \\\n",
       "0              93400               36279  36.060969 -80.184068        912   \n",
       "1             166200               51467  36.010380 -80.447340        782   \n",
       "2             111300               32820  35.337939 -82.450039       2146   \n",
       "3             110600               41340  35.291316 -80.780822        721   \n",
       "4             122200               42046  35.753580 -81.324900        969   \n",
       "\n",
       "                 ...                State.y       County          City.y  \\\n",
       "0                ...                     NC      Forsyth   Winston-Salem   \n",
       "1                ...                     NC        Davie         Advance   \n",
       "2                ...                     NC    Henderson  Hendersonville   \n",
       "3                ...                     NC  Mecklenburg       Charlotte   \n",
       "4                ...                     NC      Catawba         Hickory   \n",
       "\n",
       "            Name      AFFGEOID10 GEOID10    ALAND10  AWATER10  \\\n",
       "0      Waughtown  8600000US27107   27107  168507642   1428803   \n",
       "1    Bermuda Run  8600000US27006   27006  165428089   2643968   \n",
       "2  Mountain Home  8600000US28792   28792  260576005    716642   \n",
       "3         Newell  8600000US28213   28213   35846902    231444   \n",
       "4     Green Park  8600000US28601   28601  119209116   8448396   \n",
       "\n",
       "                                               image  \\\n",
       "0  /Users/Jon/Documents/NEWOS_DL/images/Zillow_NC...   \n",
       "1  /Users/Jon/Documents/NEWOS_DL/images/Zillow_NC...   \n",
       "2  /Users/Jon/Documents/NEWOS_DL/images/Zillow_NC...   \n",
       "3  /Users/Jon/Documents/NEWOS_DL/images/Zillow_NC...   \n",
       "4  /Users/Jon/Documents/NEWOS_DL/images/Zillow_NC...   \n",
       "\n",
       "                       image_remote  \n",
       "0   images/Zillow_NC/Zillow7787.jpg  \n",
       "1   images/Zillow_NC/Zillow8382.jpg  \n",
       "2  images/Zillow_NC/Zillow12870.jpg  \n",
       "3  images/Zillow_NC/Zillow19609.jpg  \n",
       "4  images/Zillow_NC/Zillow24954.jpg  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the dataframe\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"data/Zillow.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a complete code that will allow us to train the model using transfer learning. Don't worry if you don't understand it all right now. The main point is to give you a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input\n",
    "num_classes = 1 # how many output classes? 1 for regression\n",
    "y_col = 'popden' # name of y column in dataframe\n",
    "x_col = 'image' # name of x column in dataframe\n",
    "batch_size = 25 # how many images to process in parallel\n",
    "epochs = 1 # how long to train the model\n",
    "model_path = 'saved_models/vgg16_popden_model' # where to save the trained model\n",
    "results_path = 'output/popden.csv' # where to save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Packages...\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "print(\"Loading Packages...\")\n",
    "from tensorflow.python import keras\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Input, GlobalMaxPooling2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import sys\n",
    "sys.path.append('packages/')\n",
    "from im import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model...\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "print(\"building model...\")\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(None, None, 3), pooling='max')\n",
    "\n",
    "# Freeze the layers except the last 3 layers\n",
    "for layer in base_model.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add extra dense and dropout layers on top\n",
    "x = base_model.output\n",
    "x = Dense(128, activation='tanh')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1)(x) \n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='Adam', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "print(\"loading data...\")\n",
    "df=pd.read_csv(\"data/Zillow.csv\") # this is the dataframe that contains the image paths and response variable\n",
    "\n",
    "# load standard scaler for the response variable\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "df_train=df.sample(frac=0.75) # sample the training set\n",
    "df_test=df.drop(df_train.index) # sample the testing set\n",
    "\n",
    "std_scaler.fit(df_train[[y_col]]) # apply standard scaler\n",
    "df_train[[y_col]] = std_scaler.transform(df_train[[y_col]])\n",
    "df_test[[y_col]]=std_scaler.transform(df_test[[y_col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not actually going to run the next chunk because it trains the model, which takes a long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n",
      "Found 424 images.\n",
      "Found 142 images.\n",
      "Epoch 1/1\n",
      "1/8 [==>...........................] - ETA: 2:56:27 - loss: 4.4093 - mean_squared_error: 4.4093"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-467386dfd002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m                     callbacks = [ModelCheckpoint(model_path,\n\u001b[1;32m     45\u001b[0m                                                  \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_mean_squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#numeric use \"val_loss\", for categorical use \"val_mean_squarred_error\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                                                  save_best_only=True)])\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-sessions/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-sessions/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-sessions/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-sessions/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-sessions/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-sessions/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-sessions/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "print(\"training model...\")\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "# augment the data\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input, \n",
    "                                   rotation_range=30,        # data augmentation\n",
    "                                   horizontal_flip=True,     # data augmentation\n",
    "                                   width_shift_range = 0.2,  # data augmentation\n",
    "                                   height_shift_range = 0.2) # data augmentation\n",
    "\n",
    "train_generator=data_generator.flow_from_dataframe(dataframe=df_train, \n",
    "                                                  directory = None,\n",
    "                                                  x_col=x_col, \n",
    "                                                  y_col= y_col, \n",
    "                                                  has_ext=True, \n",
    "                                                  class_mode=\"other\",#r numeric, use \"other\", for categorical use \"categorical\"\n",
    "                                                  #target_size=(1000,1000), \n",
    "                                                  batch_size=batch_size,\n",
    "                                                  seed = 42,\n",
    "                                                  shuffle = True)\n",
    "\n",
    "valid_generator=data_generator.flow_from_dataframe(dataframe=df_test,\n",
    "                                                  directory=None,\n",
    "                                                  x_col=x_col,\n",
    "                                                  y_col= y_col,\n",
    "                                                  has_ext=True,\n",
    "                                                  class_mode=\"other\",\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  seed = 42,\n",
    "                                                  shuffle = True)\n",
    "\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=epochs,\n",
    "                    #class_weight = class_weights,\n",
    "                    callbacks = [ModelCheckpoint(model_path,\n",
    "                                                 monitor=\"val_mean_squared_error\", #numeric use \"val_loss\", for categorical use \"val_mean_squarred_error\"\n",
    "                                                 save_best_only=True)])\n",
    "\n",
    "train.extend(history.history['mean_squared_error'])\n",
    "test.extend(history.history['val_mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "print(\"saving results...\")\n",
    "\n",
    "results = pd.DataFrame(train, test) \n",
    "results.to_csv(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\"output/popden.csv\", header=0, names=['training', 'validation'])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "plt.plot(metrics['training'], label = 'training MSE')\n",
    "plt.plot(metrics['validation'], label = 'validation MSE')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Mean Squared Error', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
